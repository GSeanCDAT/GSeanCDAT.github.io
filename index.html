<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zujin GUO</title>
  
  <meta name="author" content="Zujin GUO">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zujin GUO</name>
              </p>
              <p>
                I am a research engineer in Nanyang Technological University, at , advised by <a href="https://www.mmlab-ntu.com/person/ccloy/index.html">Prof. Chen Change Loy</a>, working closely with <a href="https://weivision.github.io/">Dr. Wei Li</a>.
              </p>
              <p>  
                I received my Master degree in Artificial Intelligence at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a> in 2023 and B.Eng in Automation at <a href="https://www.xjtu.edu.cn/">XJTU</a> in 2020.
              </p>
              <p>
                My primary research focus revolves around scene understanding and motions within the field of computer vision. I have had the opportunity to contribute to the advancement of scene graph generation tasks, establishing a series of benchmarks through my collaboration with <a href="https://jingkang50.github.io/">Jingkang Yang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:gu0008in@e,ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="data\CV_Zujin GUO.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=G8DPsoUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zujin-guo-652b0417a/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/GSeanCDAT">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ZujinGuo.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ZujinGuo_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Project</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/ram_logo.png' width="160" height="70">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/Luodian/RelateAnything">
                <papertitle>Relate Anything
                </papertitle>
              </a>
              <br>
              <strong>Zujin Guo</strong>, Bo Li, Jingkang Yang, Zijian Zhou (alphabetical order)
              <br>
              <a href="https://github.com/Luodian/RelateAnything">Code</a>
              /
              <a href="https://bf5e65e511446cbe60.gradio.live/">Demo</a>
              <p></p>

            </td>
          </tr>

        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <!-- <p>
                I'm interested in computer vision and machine learning, especially in multimodal scenario's understanding and generation. 
                I also feel excited in exploring self-supervised learning methods for models on the massive data in the wild. Some of the work has been published.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pairnet.png' width="160" height="60">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf"> -->
                <papertitle>Pair then Relation: Pair-Net for Panoptic Scene Graph Generation
                </papertitle>
              </a>
              <br>
              Jinghao Wang, Zhengyu Wen, Xiangtai Li,  <strong>Zujin Guo</strong>, Jingkang Yang, Ziwei Liu
              <br>
              <em>arxiv</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2307.08699.pdf">Paper</a>
              /
              Code
              <p></p>
              <!-- <p>
                The Panoptic Scene Graph Generation (PSG) Task aims to interpret a complex scene image with a scene graph representation, with each node in the scene graph grounded by its pixel-accurate segmentation mask in the image. We established the first PSG dataset together with two-stage and one-stage baselines for it.
              </p> -->
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="pvsg_stop()" onmouseover="pvsg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/pvsg.png' width="160" height="109">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">
                <papertitle>Panoptic Video Scene Graph Generation
                </papertitle>
              </a>
              <br>
              Jingkang Yang, Wenxuan Peng, Xiangtai Li, <strong>Zujin Guo</strong>, Liangyu Chen, Bo Li, Zheng Ma, Wayne Zhang, Kaiyang Zhou, Chen Change Loy, Ziwei Liu
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">Paper</a>
              /
              Code
              <p></p>
              <!-- <p>
                The Panoptic Scene Graph Generation (PSG) Task aims to interpret a complex scene image with a scene graph representation, with each node in the scene graph grounded by its pixel-accurate segmentation mask in the image. We established the first PSG dataset together with two-stage and one-stage baselines for it.
              </p> -->
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="psg_stop()" onmouseover="psg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/psg_data.jpg' width="160" height="109">
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.11247">
                <papertitle>Panoptic Scene Graph Generation
                </papertitle>
              </a>
              <br>
              Jingkang Yang,
              Yi Zhe Ang, 
              <strong>Zujin Guo</strong>,
              Kaiyang Zhou,
              Wayne Zhang
              Ziwei Liu
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870175.pdf">Paper</a>
              /
              <a href="https://github.com/Jingkang50/OpenPSG">Code</a>
              /
              <a href="https://psgdataset.org/">Project page</a>
              /
              <a href="https://huggingface.co/spaces/ECCV2022/PSG">Demo</a>
              <p></p>
              <!-- <p>
                The Panoptic Scene Graph Generation (PSG) Task aims to interpret a complex scene image with a scene graph representation, with each node in the scene graph grounded by its pixel-accurate segmentation mask in the image. We established the first PSG dataset together with two-stage and one-stage baselines for it.
              </p> -->
            </td>
          </tr>

        </tbody></table>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Code&Dataset</heading>
              
            </td>
          </tr>

        </tbody></table>
		
        <ul>
          <li><a href="https://psgdataset.org/"><strong>PSG</strong></a>: Dataset and Benchmark for Panoptic Scene Graph Generation (PSG), ECCV'22</li>
          <li><a href="https://github.com/Luodian/RelateAnything"><strong>RAM</strong></a>: Code for Relate Anything</li>
        </ul>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last update: May 2023  &nbsp&nbsp&nbsp&nbsp <a href="https://github.com/jonbarron/website">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<a href="https://clustrmaps.com/site/1buwe"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=VY6Ce1BJw7P2sTnIFDrQLoa1Jrwfdgw6TCu9JNHqFRQ&cl=ffffff" /></a>

</body>

</html>
